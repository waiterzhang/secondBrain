# Zookeeper

zk是一个开源的分布式协调服务。

> **带着问题去学习。**
>
> “分布式”的思路给系统的弹性和稳定性都带来了更多的可能性，但是“分布式”本身也带来了一系列问题：
>
> * 数据一致性问题：分布式系统中多个节点之间的读写操作之后，如何保证各个节点之间数据的一致性？
> * 并发访问控制：当多个进程、线程试图同时访问hove修改共享资源的时候，如何协调这些并发操作，以避免竞争条件和死锁问题？
> * 服务发现和动态配置：在分布式系统中，服务的实例会动态的加入或者离开，配置信息也可能随时发生变化。如何保证及时、准确的更新信息？
> * 等等
>
> 对于上述问题，zk给了自己的解决方案。

## 基本概念

### 设计目标

**zk结构简单**

zk运行应用通过共享的分层命名空间相互协调，该命名空间与文件系统类似。命名空间由数据节点（znode），它类似于文件和目录。zk的数据保存在内存中，因此zk可以实现高吞吐和低延迟。

**zk是可复制的**

zk支持集群部署。zk集群的各个节点之间相互发现，节点之间的状态信息维护在内存中。只要大部分节点可用，zk服务就可用。

​![image](image-20240413180605-rqgmx9u.png)​

**zk是有序的**

zk通过事务的顺序来标记每个更新（类似于版本），顺序序号可以用来实现更复杂的操作，例如同步原语。

**zk非常快**

zk速度很快，尤其是在读大于写的环境，在十读一写的比例下性能最高，

### 分层命名空间

zk的命名空间和文件系统很像，通过斜杠分隔路径元素，zk命名空间中的每个节点都由路径标识。zk的分层命名空间：

​![image](image-20240413180719-ertnfd7.png)​

### 节点和临时节点

节点（znode）是zk中的核心概念。

* Znode节点通过树形结构组织，每个节点都有其唯一的路径名称，这种结构化的数据组织方式可以使得用户可以通过路径名称访问和操作特定的的节点。

* Znode节点中存储和维护元数据，元数据包括版本号（节点每次变化，版本号都会增加）、ACL（访问控制列表，即谁可以访问什么内容）、创建时间、修改时间等信息。通过这些信息可以实现Watch机制、版本控制等功能。  
  ​![image](image-20240413185702-3ucqktk.png)​

* Znode节点发生变化的时候，zk会触发相应的时间通知给客户端。这种机制使得客户端能够实时监测分布式系统状态的变化。

> 不准确地说，节点可以理解为服务。（并非一个节点对应一个服务；同时节点里面存在的是服务方的信息，节点本身并不会提供“服务”）
>
> 举一个例子：
>
> 假设我们有一个分布式应用，其中涉及多个服务提供者（例如，多个数据库服务器）和多个服务消费者（例如，多个应用服务器）。在这个场景中，Zookeeper和Znode可以用来实现服务的注册、发现和动态负载均衡。
>
> 1. **服务注册**：每个服务提供者启动时，会在Zookeeper中创建一个Znode节点。这个Znode节点可以包含服务提供者的相关信息，如IP地址、端口号、服务类型等。例如，一个数据库服务器可能在Zookeeper中创建一个路径为`/services/database/server1`​的Znode节点，并将自身的相关信息存储在该节点中。
> 2. **服务发现**：服务消费者启动时，会连接到Zookeeper并订阅相关的Znode节点。它们可以查询这些节点以获取可用的服务提供者列表。在上面的例子中，应用服务器可能会查询`/services/database`​路径下的所有节点，从而发现所有注册的数据库服务器。

zk中还有临时节点的概念。临时节点的生命周期与会话强相关：当创建临时节点的会话存在，znode就会存在；会话结束，znode会被自动删除。

> 为什么要有临时节点？
>
> 业务或者信息有其生命周期，自然要提供匹配对应生命周期的信息载体，不然就会造成资源的浪费。那么很显然，长期不变动的信息适合存放在永久节点，强生命周期变化的信息适合放在临时节点。

### 条件更新与监控

zk支持watch的概念。客户端可以在znode上设置watch，当znode发生变化的时候，watch将会被触发并移除。当处罚watch的时候，客户端会收到一个数据包，表明znode已经更改。

在zk的3.6.0中新增了永久的、递归的watch，这些watch会被递归处罚，并且不会被移除。

> 可以理解为znode更新的回调。

### ZK的承诺

* 顺序一致性：来自客户端的更新将按照发送顺序更新。
* 原子性：要么全部成功、要么全部失败。
* 全局状态一致：无论客户端连接哪个zk服务器，客户端都会看到相同的服务状态。
* 及时性：保证客户端视图在一定时间范围内是最新的。
* 可靠性：Once an update has been applied, it will persist from that time forward until a client overwrites the update.

  > 没理解
  >

## 数据一致性

读写操作：

* 每个节点都可以处理读请求，优先读取节点自身缓存。

* 写：写被视为事务。只有leader能够处理写请求。当客户端需要写入数据时，写入请求会被转发到leader节点，leader节点处理该请求，并将数据变更传播到其他的字节点。一旦超过一半的子节点成功进行了本次更新，那么集群就认为这次写入是成功的。

> 带着问题去学习 **：**
>
> * 读取缓存如何知道当前的缓存是最新的？
> * 只有过半的节点成功应用更新就认为写入成功，那如何保证不同节点之间数据读取的一致性？
> * lead崩溃了如何恢复？
> * leader恢复后如何进行数据同步？

### ZAB协议

**核心思路**

zab协议中有三种角色：

* leader：一个集群同一时间只有一个实际工作的leader，他维护与各个follwer即observer之间的心跳。所有的写操作都会被转发到leader完成。
* follower：可直接处理客户端的读请求，同时将写请求转发给leader处理，并具有投票权。
* observer：与follower类似，但没有投票权。

  > 为什么要有observer？
  >
  > 投票本身是比较耗时的，太多的具有投票权的节点会增加投票的耗时
  >

所有事务请求必须由⼀个全局唯⼀的服务器来协调处理，这样的服务器被称为Leader服务器，余下的服务器则称为Follower服务器， Leader服务器负责将⼀个客户端事务请求转化成⼀个事务Proposal（提议），并将该Proposal分发给集群中所有的Follower服务器，之后Leader服务器需要等待所有Follower服务器的反馈，⼀旦超过半数的Follower服务器进⾏了正确的反馈后，那么Leader就会再次向所有的Follower服务器分发Commit消息，要求其将前⼀个Proposal进⾏提交。

zab协议主要包括两种模式：消息广播和故障恢复

**消息广播**

当集群中有过半的follower服务器完成了与leader服务器的状态同步，那么整个集群处于消息广播模式。当有新的节点加入集群之后，如果leader正常存在，则新节点会进入数据恢复模式。

leader通过广播模式保证各个follower节点的数据一致性

​![image](image-20240414110437-a4v1z9c.png)​

* zk中的每个节点都拥有数据的完整副本，可以独立处理读请求。
* 当follower收到写请求，会转发给leader。只有leader可以处理写请求。
* leader会为每一个写请求分配全局有序的唯一事务id（zookeeper transaction id，zxid），并转化为事务proposal提案发送到消息队列（每一个follower都有自己先进先出的消息队列，这保证了事务的顺序）。见黑色1号箭头。
* follower从队列中取出提案之后写到事务日志（本地磁盘），但不提交。随后ack告知leader可以进行提交。见紫红色2号箭头。
* leader收到过半follower的ac之后（leader自己默认有一个ack），发出commit请求执行。见红色3号箭头。
* 剩余未完成事务的follower会放弃执行此事务，进入数据同步阶段。

  > 我猜：数据同步阶段的节点读取受限，因为要保证对外服务的一致性。
  >

  * follower想leader发送followerInfo（含有当前节点的last的zxid），然后follower发送newleader信息。信息主要包括三部分：

    * snap：如果follower数据太老，leader将会发送snap指令向follower同步数据。全量更新？
    * diff：leader发送follower-lastZxid到leader-lastZxid议案的diff指令给follower同步数据。增加更新？
    * trunc：当follow-lastZxit比leader-lastZxid大，leader会发送trunc指令让follower丢弃该段数据。

> 消息广播总结：
>
> * 两阶段提交；
> * 半数ack即认为写入成功，随后进入数据同步模式；
> * 数据的新旧通过事务id（zxid）来判断；
> * 数据以leader为准，follower同步有三种操作：全量更新、增量更新、放弃超过leader的部分以保持全局数据一致；
> * 写操作做为事务放入每一个follower的队列，保证有序。
> * 未完成同步的节点读取受限，以保证提供的服务一致 **（我猜的）**

‍

**故障恢复**

集群启动或者leader崩溃的时候进入恢复模式，故障恢复就是要投票选出新的leader节点。

选票数据结构

* logicClock：每个服务器维护一个自增数据，用来表示这是该服务器发起的第多少轮投票；
* stat：服务器当前的状态；
* mid：当前服务器的id；
* zxid：当前服务器所保存的数据的最大的zxid（重要，标识数据的新旧）；
* vote-id：被推举服务器的id
* vote-zxid：被推举服务器所保存的数据的最大id；

> 从以上数据结构可以大致猜到选举的原则：数据最新的当选。

选举流程：

* logicClock自增，标识一次新的投票。每个服务器最开始都是把票投给自己；
* 接收外部选票，通过logicClock判断当前选举的轮次是否合适（不合适：落后？超前？），然后做出不同的反应（更新自己？忽略）。如果轮次有效，就将票投给vote-zxid最大的那位。如果vote-zxid相同，则优选vote-id更大的那位。并将票广播出去。
* 如果有过半服务器认可自己的投票，则终止投票。（再投也没啥意义，因为过半肯定会成功）否则继续接收其他服务器的投票。
* 归票：若过半的票投给了自己，则将服务器状态更新为leading，否则更新为following；
* leading选举成功之后，就应该同步数据了。（我猜）

> 故障恢复总结：
>
> * 选举原则：数据最新的当leader

‍

## 回答问题

* 数据一致性问题：分布式系统中多个节点之间的读写操作之后，如何保证各个节点之间数据的一致性？

  > 两阶段提交+数据同步+未同步节点访问受限
  >
* 并发访问控制：当多个进程、线程试图同时访问hove修改共享资源的时候，如何协调这些并发操作，以避免竞争条件和死锁问题？

  > 事务+消息队列保证FIFO
  >
* 服务发现和动态配置：在分布式系统中，服务的实例会动态的加入或者离开，配置信息也可能随时发生变化。如何保证及时、准确的更新信息？

  > watch机制
  >

* 读取缓存如何知道当前的缓存是最新的？

  > watch机制
  >
* 只有过半的节点成功应用更新就认为写入成功，那如何保证不同节点之间数据读取的一致性？

  > 数据同步 + 未同步节点访问受限
  >
* lead崩溃了如何恢复？

  > 投票（数据新者优先）
  >
* leader恢复后如何进行数据同步？

  > follower发送数据（zxid）状态，对比状态后全量/增量/丢弃更新。数据以leader为准。
  >
